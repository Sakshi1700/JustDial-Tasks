{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scaping - Dominos.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "By Sakshi Verma"
      ],
      "metadata": {
        "id": "E0jlunsOGhUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# Define the base URL\n",
        "base_url = \"https://www.consumeraffairs.com/food/dominos.html\"\n",
        "\n",
        "# Create an empty list to store all review\n",
        "all_pages_reviews =[]\n",
        "\n",
        "# Create a Scraper function\n",
        "def scraper():\n",
        "\t# Web scraping - fetching the reviews from the webpage using BeautifulSoup\n",
        "\n",
        "\t# loop through a range of page numbers \n",
        "\tfor i in range(1,6): # fetching reviews from five pages\n",
        "\n",
        "\t\t# Creating an empty list to store the reviews of each page\n",
        "\t\tpagewise_reviews = [] \n",
        "\n",
        "\t\t# Query parameter\n",
        "\t\tquery_parameter = \"?page=\"+str(i)\n",
        "\n",
        "\t\t# Constructing the URL\n",
        "\t\turl = base_url + query_parameter\n",
        "\t\t\n",
        "\t\t# Send HTTP request to the URL\n",
        "\t\tresponse = requests.get(url)\n",
        "\n",
        "\t\t# Create a soup object and parse the HTML page\n",
        "\t\tsoup = bs(response.content, 'html.parser') \n",
        "\n",
        "\t\t# Finding all the elements having reviews using class attribute\n",
        "\t\trev_div = soup.findAll(\"div\",attrs={\"class\",\"rvw-bd\"}) \n",
        "\n",
        "\t\t# loop through all the divs and append \n",
        "\t\tfor j in range(len(rev_div)):\n",
        "\t\t\t# finding all the p tags to fetch only the review text\n",
        "\t\t\tpagewise_reviews.append(rev_div[j].find(\"p\").text)\n",
        "\n",
        "\t\t# writing all the reviews into a list\n",
        "\t\tfor k in range(len(pagewise_reviews)):\n",
        "\t\t\tall_pages_reviews.append(pagewise_reviews[k]) \n",
        "\n",
        "\t# return the final list of reviews\n",
        "\treturn all_pages_reviews\n",
        "\n",
        "# Driver code\n",
        "reviews = scraper()\n",
        "\n",
        "# Storing in a dataframe\n",
        "i = range(1, len(reviews)+1)\n",
        "reviews_df = pd.DataFrame({'review':reviews}, index=i)\n",
        "\n",
        "# Writing to a text file\n",
        "reviews_df.to_csv('reviews.txt', sep='\\t')"
      ],
      "metadata": {
        "id": "JaP-dRSKmylf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IEqUnOwPm-4T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}